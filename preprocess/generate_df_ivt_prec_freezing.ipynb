{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This preprocessing notebook takes 6-hourly AR data from tARget v3, and selects only the times where an AR crosses the 1 km threshold in HMA.\n",
    "\n",
    "Using the trackID from "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Python modules\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "import metpy.calc as mpcalc\n",
    "from metpy.units import units\n",
    "\n",
    "# import personal modules\n",
    "\n",
    "# Path to modules\n",
    "sys.path.append('../modules')\n",
    "\n",
    "# Import my modules\n",
    "from ar_funcs import get_topo_mask\n",
    "from timeseries import select_months_ds, select_months_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "server = 'great'\n",
    "path_to_data = '/home/nash/DATA/data/'                                      # project data -- read only\n",
    "path_to_out  = '../out/'       # output files (numerical results, intermediate datafiles) -- read & write\n",
    "path_to_figs = '../figs/'      # figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of AR dates and trackIDs when an AR crosses 1000 m elevation threshold in HMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify ARs using single bound box with elevation mask during DJF\n",
    "bbox = [20, 40, 65, 97] # HMA region\n",
    "elev_thres = 1000.\n",
    "ssn = 'MAM'\n",
    "\n",
    "if ssn == 'DJF':\n",
    "    start_date = '1979-12-01 0:00'\n",
    "    end_date = '2015-02-28 18:00'\n",
    "    start_mon = 12\n",
    "    end_mon = 2\n",
    "if ssn == 'MAM':\n",
    "    start_date = '1980-03-01 0:00'\n",
    "    end_date = '2014-05-31 18:00'\n",
    "    start_mon = 3\n",
    "    end_mon = 5\n",
    "\n",
    "# open ds\n",
    "filename =  path_to_data + 'ar_catalog/globalARcatalog_ERA-Interim_1979-2019_v3.0.nc'\n",
    "ds = xr.open_dataset(filename, chunks={'time': 1460}, engine='netcdf4')\n",
    "ds = ds.squeeze()\n",
    "# remove lev and ens coords\n",
    "ds = ds.reset_coords(names=['lev', 'ens'], drop=True)\n",
    "\n",
    "# select lats, lons, and dates within start_date, end_date and months\n",
    "lat1, lat2, lon1, lon2 = bbox\n",
    "ds = ds.sel(time=slice(start_date, end_date), lat=slice(lat1,lat2), lon=slice(lon1,lon2))\n",
    "ds = select_months_ds(ds, start_mon, end_mon)\n",
    "\n",
    "# add topo mask\n",
    "mask = get_topo_mask(ds.lat, ds.lon) # create a elevation dataset with same grid spacing as ds\n",
    "ds = ds.where(mask.bedrock >= elev_thres) # mask ds where elevation is less than 1000 m\n",
    "\n",
    "# convert dataset to dataframe\n",
    "df = ds.kidmap.to_dataframe(dim_order=['time', 'lat', 'lon'])\n",
    "df = df.dropna(axis='rows')\n",
    "# keep only rows that have trackID\n",
    "trackID = df.groupby('time').kidmap.unique()\n",
    "# trackID # this is all trackIDs that crossed the 1000 m threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_df = trackID.to_frame() # converts to a pandas dataframe\n",
    "id_df = id_df.reset_index() # reset the index\n",
    "id_df = id_df.rename(columns={'time': 'date'}) # rename time column into date\n",
    "id_df = id_df.set_index(pd.to_datetime(id_df['date'])) # reset the index as \"date\"\n",
    "id_df.index = id_df.index.strftime(\"%Y-%m-%d\") # make it so the index date is normalized to daily\n",
    "id_df = id_df.rename(columns={'date': 'time'}) # rename the date column back to time\n",
    "id_df = id_df.reset_index() # remove the index\n",
    "id_df = id_df.explode('kidmap') # explode the dataframe based on trackID\n",
    "# id_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load AR CAT (from Nash et al. 2021)\n",
    "filepath = path_to_out + 'AR-types_ALLDAYS.csv'\n",
    "ar_cat = pd.read_csv(filepath)\n",
    "ar_cat = ar_cat.rename(columns={'Unnamed: 0': 'date'})\n",
    "ar_cat = ar_cat.set_index(pd.to_datetime(ar_cat['date']))\n",
    "ar_cat = select_months_df(ar_cat, start_mon, end_mon)\n",
    "ar_cat.index = ar_cat.index.strftime(\"%Y-%m-%d\")\n",
    "ar_cat = ar_cat.drop(columns=['date'])\n",
    "ar_cat = ar_cat.reset_index()\n",
    "idx = ar_cat['AR_CAT'] > 0\n",
    "ar_cat = ar_cat.loc[idx]\n",
    "\n",
    "# ar_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge id_df with ar_cat\n",
    "merge_ar = pd.merge(id_df, ar_cat, how='outer', on='date')\n",
    "track_ids = merge_ar.kidmap.unique() # get unique list of AR track IDs\n",
    "ar_dates = merge_ar.time.unique() # get unique list of AR date/times (for later)\n",
    "# merge_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trackID</th>\n",
       "      <th>ar_cat</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3579.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1980-03-03 00:00:00</td>\n",
       "      <td>1980-03-04 06:00:00</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3603.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1980-03-05 06:00:00</td>\n",
       "      <td>1980-03-06 00:00:00</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3719.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1980-03-20 00:00:00</td>\n",
       "      <td>1980-03-21 00:00:00</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>6760.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1981-03-04 00:00:00</td>\n",
       "      <td>1981-03-04 18:00:00</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>6783.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1981-03-04 18:00:00</td>\n",
       "      <td>1981-03-05 06:00:00</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3605</th>\n",
       "      <td>111955.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2014-05-13 06:00:00</td>\n",
       "      <td>2014-05-13 12:00:00</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3610</th>\n",
       "      <td>112035.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2014-05-23 06:00:00</td>\n",
       "      <td>2014-05-23 12:00:00</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3611</th>\n",
       "      <td>112080.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2014-05-27 06:00:00</td>\n",
       "      <td>2014-05-28 00:00:00</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3612</th>\n",
       "      <td>112083.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2014-05-28 06:00:00</td>\n",
       "      <td>2014-05-28 12:00:00</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3613</th>\n",
       "      <td>112092.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2014-05-30 18:00:00</td>\n",
       "      <td>2014-05-31 00:00:00</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1374 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       trackID  ar_cat          start_date            end_date  duration\n",
       "0       3579.0       1 1980-03-03 00:00:00 1980-03-04 06:00:00      30.0\n",
       "2       3603.0       1 1980-03-05 06:00:00 1980-03-06 00:00:00      18.0\n",
       "9       3719.0       1 1980-03-20 00:00:00 1980-03-21 00:00:00      24.0\n",
       "34      6760.0       1 1981-03-04 00:00:00 1981-03-04 18:00:00      18.0\n",
       "35      6783.0       1 1981-03-04 18:00:00 1981-03-05 06:00:00      12.0\n",
       "...        ...     ...                 ...                 ...       ...\n",
       "3605  111955.0       3 2014-05-13 06:00:00 2014-05-13 12:00:00       6.0\n",
       "3610  112035.0       3 2014-05-23 06:00:00 2014-05-23 12:00:00       6.0\n",
       "3611  112080.0       3 2014-05-27 06:00:00 2014-05-28 00:00:00      18.0\n",
       "3612  112083.0       3 2014-05-28 06:00:00 2014-05-28 12:00:00       6.0\n",
       "3613  112092.0       3 2014-05-30 18:00:00 2014-05-31 00:00:00       6.0\n",
       "\n",
       "[1374 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create df with trackID, ar_cat, start date, end date, and duration of AR (how long it is within HMA region)\n",
    "ar = []\n",
    "data = []\n",
    "for i in [1, 2, 3]:\n",
    "    idx = (merge_ar.AR_CAT == i)\n",
    "    ar = merge_ar.loc[idx]\n",
    "\n",
    "    for j, ids in enumerate(track_ids):\n",
    "        idx = (ar.kidmap == ids)\n",
    "        tmp = ar.loc[idx]\n",
    "        start = pd.to_datetime(tmp.time.min())\n",
    "        stop = pd.to_datetime(tmp.time.max()) + timedelta(hours=6)\n",
    "        tmp = (stop - start)\n",
    "        duration = tmp.total_seconds()/(3600) # convert to number of hours\n",
    "\n",
    "        data.append([ids, i, start, stop, duration])\n",
    "    \n",
    "duration_df = pd.DataFrame(data, columns=['trackID', 'ar_cat', 'start_date', 'end_date', 'duration'])\n",
    "duration_df = duration_df.dropna()\n",
    "duration_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landslide DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expand_grid(lat,lon):\n",
    "    '''list all combinations of lats and lons using expand_grid(lat,lon)'''\n",
    "    test = [(A,B) for A in lat for B in lon]\n",
    "    test = np.array(test)\n",
    "    test_lat = test[:,0]\n",
    "    test_lon = test[:,1]\n",
    "    full_grid = pd.DataFrame({'lat': test_lat, 'lon': test_lon})\n",
    "    full_grid = full_grid.sort_values(by=['lat','lon'])\n",
    "    full_grid = full_grid.reset_index(drop=True)\n",
    "    return full_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fname = path_to_data + 'CH2_generated_data/Global_Landslide_Catalog_Export.csv' #TODO check this - is it the raw downloaded data?\n",
    "landslide = pd.read_csv(fname)\n",
    "\n",
    "# Select lat/lon grid\n",
    "lonmin = 65\n",
    "lonmax = 100\n",
    "latmin = 20\n",
    "latmax = 42\n",
    "\n",
    "## Select Landslides within Southern Asia region\n",
    "idx = (landslide.latitude >= latmin) & (landslide.latitude <= latmax) & (landslide.longitude >= lonmin) & (landslide.longitude <= lonmax)\n",
    "landslide = landslide.loc[idx]\n",
    "# set event time as index\n",
    "landslide = landslide.set_index(pd.to_datetime(landslide.event_date))\n",
    "# landslide.index = landslide.index.normalize()\n",
    "\n",
    "# select only landslide dates that are between december and may\n",
    "idx = (landslide.index.month >= 12) | (landslide.index.month <= 5)\n",
    "landslide = landslide[idx]\n",
    "\n",
    "# rename and reindex\n",
    "landslide = landslide.rename(columns={\"latitude\": \"lat\", \"longitude\": \"lon\", \"event_date\": \"event_time\"})\n",
    "landslide = landslide.reset_index()\n",
    "\n",
    "# round event time to the nearest 6 hours\n",
    "landslide['time'] = landslide['event_date'].dt.round('6H')\n",
    "landslide = landslide.set_index(pd.to_datetime(landslide.time))\n",
    "\n",
    "# select only landslide dates that are between december and may\n",
    "idx = (landslide.index.month >= 12) | (landslide.index.month <= 5)\n",
    "landslide = landslide[idx]\n",
    "\n",
    "# landslide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now we want to see if there is an AR present at the same time and location as the landslides\n",
    "# open the trackID for ARs\n",
    "filename =  path_to_data + 'ar_catalog/globalARcatalog_ERA-Interim_1979-2019_v3.0.nc'\n",
    "ar = xr.open_dataset(filename, engine='netcdf4')\n",
    "ar = ar.squeeze()\n",
    "\n",
    "# Select months\n",
    "idx = (ar.time.dt.month >= 12) | (ar.time.dt.month <= 5)\n",
    "kid = ar.kidmap.sel(time=idx) # trackID for indexing\n",
    "\n",
    "# slice the dates so both ds match\n",
    "kid = kid.sel(time=slice('1979-12-01 00', '2019-05-31 00:00'))\n",
    "# kid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## for each landslide_id, if the lat/lon falls within an AR, keep that AR ID and landslide ID\n",
    "landslideID = []\n",
    "arID = []\n",
    "landslide_lat = []\n",
    "landslide_lon = []\n",
    "for i, row in landslide.T.iteritems():\n",
    "    t = kid.sel(lat=row['lat'], lon=row['lon'], time=row['time'], method='nearest').values\n",
    "    # print(t)\n",
    "    if t > 0:\n",
    "        landslideID.append(row['event_id'])\n",
    "        arID.append(t)\n",
    "        landslide_lat.append(row['lat'])\n",
    "        landslide_lon.append(row['lon'])\n",
    "        \n",
    "d = {'landslideID': landslideID, 'trackID': arID, \n",
    "     'landslide_lat': landslide_lat, 'landslide_lon': landslide_lon}\n",
    "landslide_df = pd.DataFrame(data=d)\n",
    "# convert the dtype for the trackID column\n",
    "landslide_df = landslide_df.astype({'trackID': 'float64'})\n",
    "\n",
    "# landslide_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge AR duration df and landslide DF\n",
    "merged_data = pd.merge(duration_df, landslide_df, how='outer', on='trackID')\n",
    "# merged_data \n",
    "# note the rows that do not have a date or time \n",
    "# are landslides that are associated with a specific AR that was not considered a \"HMA AR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## test to make sure merged correctly\n",
    "# idx = merged_data.landslideID > 0\n",
    "# test = merged_data[idx]\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trackID</th>\n",
       "      <th>ar_cat</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>duration</th>\n",
       "      <th>landslideID</th>\n",
       "      <th>landslide_lat</th>\n",
       "      <th>landslide_lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3579.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1980-03-03 00:00:00</td>\n",
       "      <td>1980-03-04 06:00:00</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3579.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1980-03-01 00:00:00</td>\n",
       "      <td>1980-03-03 00:00:00</td>\n",
       "      <td>48.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3603.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1980-03-05 06:00:00</td>\n",
       "      <td>1980-03-06 00:00:00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3719.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1980-03-20 00:00:00</td>\n",
       "      <td>1980-03-21 00:00:00</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3719.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1980-03-19 06:00:00</td>\n",
       "      <td>1980-03-20 00:00:00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1381</th>\n",
       "      <td>111955.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2014-05-13 06:00:00</td>\n",
       "      <td>2014-05-13 12:00:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1382</th>\n",
       "      <td>112035.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2014-05-23 06:00:00</td>\n",
       "      <td>2014-05-23 12:00:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>112080.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2014-05-27 06:00:00</td>\n",
       "      <td>2014-05-28 00:00:00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384</th>\n",
       "      <td>112083.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2014-05-28 06:00:00</td>\n",
       "      <td>2014-05-28 12:00:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>112092.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2014-05-30 18:00:00</td>\n",
       "      <td>2014-05-31 00:00:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1386 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       trackID  ar_cat          start_date            end_date  duration  \\\n",
       "0       3579.0     1.0 1980-03-03 00:00:00 1980-03-04 06:00:00      30.0   \n",
       "1       3579.0     2.0 1980-03-01 00:00:00 1980-03-03 00:00:00      48.0   \n",
       "2       3603.0     1.0 1980-03-05 06:00:00 1980-03-06 00:00:00      18.0   \n",
       "3       3719.0     1.0 1980-03-20 00:00:00 1980-03-21 00:00:00      24.0   \n",
       "4       3719.0     2.0 1980-03-19 06:00:00 1980-03-20 00:00:00      18.0   \n",
       "...        ...     ...                 ...                 ...       ...   \n",
       "1381  111955.0     3.0 2014-05-13 06:00:00 2014-05-13 12:00:00       6.0   \n",
       "1382  112035.0     3.0 2014-05-23 06:00:00 2014-05-23 12:00:00       6.0   \n",
       "1383  112080.0     3.0 2014-05-27 06:00:00 2014-05-28 00:00:00      18.0   \n",
       "1384  112083.0     3.0 2014-05-28 06:00:00 2014-05-28 12:00:00       6.0   \n",
       "1385  112092.0     3.0 2014-05-30 18:00:00 2014-05-31 00:00:00       6.0   \n",
       "\n",
       "      landslideID  landslide_lat  landslide_lon  \n",
       "0             NaN            NaN            NaN  \n",
       "1             NaN            NaN            NaN  \n",
       "2             NaN            NaN            NaN  \n",
       "3             NaN            NaN            NaN  \n",
       "4             NaN            NaN            NaN  \n",
       "...           ...            ...            ...  \n",
       "1381          NaN            NaN            NaN  \n",
       "1382          NaN            NaN            NaN  \n",
       "1383          NaN            NaN            NaN  \n",
       "1384          NaN            NaN            NaN  \n",
       "1385          NaN            NaN            NaN  \n",
       "\n",
       "[1386 rows x 8 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the rows that are not a HMA AR\n",
    "idx = merged_data['ar_cat'] > 0\n",
    "merged_data = merged_data.loc[idx]\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load 2D WRF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pull wrflats and wrflons from first file\n",
    "fname = path_to_data + 'wrf_hasia/d01/ivt/3hr/tmp_2015.nc'\n",
    "tmp = xr.open_dataset(fname)\n",
    "# print(tmp.time[:100])\n",
    "# print(tmp.time[-100:])\n",
    "\n",
    "## assign those lats to the other ds when you loop\n",
    "wrflats = tmp.lat.values\n",
    "wrflons = tmp.lon.values\n",
    "\n",
    "fname = path_to_data + 'wrf_hasia/d02/prec/3hr/tmp_2014.nc'\n",
    "tmp = xr.open_dataset(fname)\n",
    "# print(tmp.time[:100])\n",
    "# print(tmp.time[-100:])\n",
    "\n",
    "## assign those lats to the other ds when you loop\n",
    "wrflats2 = tmp.lat.values\n",
    "wrflons2 = tmp.lon.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ivt\n",
      "prec\n",
      "CPU times: user 1.76 s, sys: 50.2 ms, total: 1.81 s\n",
      "Wall time: 1.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def preprocess_ivt(ds):\n",
    "    '''keep only the current year'''\n",
    "    year = ds.time.dt.year.max().values\n",
    "    ds = ds.assign_coords({\"lon\": wrflons, \"lat\": wrflats})\n",
    "    if year == 1980:\n",
    "        ds = ds\n",
    "    else:\n",
    "        ds = ds.sel(time=slice('{0}-01-01 00:00'.format(year), '{0}-12-31 21:00'.format(year)))\n",
    "    return ds\n",
    "\n",
    "def preprocess_prec(ds):\n",
    "    '''keep only the current year'''\n",
    "    year = ds.time.dt.year.max().values\n",
    "    ds = ds.assign_coords({\"lon\": wrflons2, \"lat\": wrflats2})\n",
    "    if year == 1980:\n",
    "        ds = ds\n",
    "    else:\n",
    "        ds = ds.sel(time=slice('{0}-01-01 00:00'.format(year), '{0}-12-31 21:00'.format(year)))\n",
    "    return ds\n",
    "\n",
    "domains = ['d01', 'd02']\n",
    "varname_lst = ['ivt', 'prec']\n",
    "\n",
    "## loop through each ds\n",
    "ds_lst = []\n",
    "for i, (dom, varname) in enumerate(zip(domains, varname_lst)):\n",
    "    print(varname)\n",
    "    if server == 'great':\n",
    "        data_path = path_to_data + 'wrf_hasia/'\n",
    "    else:\n",
    "        data_path = path_to_data + 'wrf_preprocessed_data/wrf_6km/'\n",
    "        \n",
    "    filename_pattern = '{0}/{1}/3hr/tmp_*.nc'.format(dom, varname)\n",
    "    fname = data_path + filename_pattern\n",
    "    \n",
    "    if varname == 'ivt':\n",
    "        ds = xr.open_mfdataset(fname, preprocess=preprocess_ivt)\n",
    "        ds = ds.assign(ivt=lambda ds: np.sqrt(ds.ivtu**2 + ds.ivtv**2))\n",
    "    elif varname == 'prec':\n",
    "        ds = xr.open_mfdataset(fname, preprocess=preprocess_prec)\n",
    "        ## shift subtraction to get mm per hour \n",
    "        # # rain at next time step - rain at current time step\n",
    "        ds = ds.shift(time=-1) - ds # if in xarray\n",
    "    elif varname == 'geopotential':\n",
    "        ds = ds.sel(lev=250.)\n",
    "    \n",
    "    # subset to just ar days\n",
    "    # ds = ds.sel(time = slice(start_date, end_date))\n",
    "    # ds = select_months_ds(ds, start_mon, end_mon)\n",
    "    ds = ds.sel(time = ar_dates[:-1])\n",
    "    \n",
    "    ds_lst.append(ds)\n",
    "    \n",
    "ivt = ds_lst[0]\n",
    "prec = ds_lst[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Having trouble with wrf ds not having all the dates in the ar_dates list \n",
    "# ## use this to find out which dates are having a problem\n",
    "# # make a pandas dataframe of AR Dates\n",
    "# d = {'dates': ar_dates[:-1]}\n",
    "# df_A = pd.DataFrame(data=d)\n",
    "# df_A = df_A.set_index(pd.to_datetime(df_A['dates'])) # reset the index as \"dates\"\n",
    "\n",
    "\n",
    "# # make a pandas dataframe of WRF dates\n",
    "# d = {'dates': ivt.time}\n",
    "# df_B = pd.DataFrame(data=d)\n",
    "# df_B = df_B.set_index(pd.to_datetime(df_B['dates'])) # reset the index as \"dates\"\n",
    "\n",
    "# # test = df_A.isin(df_B)\n",
    "\n",
    "# x = df_A.index\n",
    "# y = df_B.index\n",
    "# test = x.isin(y)\n",
    "\n",
    "# idx = (test== False)\n",
    "# df_A.loc[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latmin, latmax, lonmin, lonmax\n",
    "ext1 = [71, 79, 32, 37] # Western precip anomalies\n",
    "ext2 = [69, 74, 37, 40] # Northwestern precip anomalies\n",
    "ext3 = [90, 99, 24, 30] # Eastern precip anomalies\n",
    "\n",
    "region_name = ['western', 'northwestern', 'eastern']\n",
    "domains = [ext1, ext2, ext3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # make a ds for each subregion\n",
    "# ds_lst = []\n",
    "# for i, bnds in enumerate(domains):\n",
    "#     tmp = ivt.sel(lat=slice(bnds[2], bnds[3]), lon=slice(bnds[0], bnds[1])) \n",
    "#     ds_lst.append(tmp)\n",
    "# ds_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ar_ivt(df, ds, domains):\n",
    "    '''Calculate maximum IVT for a subregion in a ds and append to dataframe.\n",
    "     For each range of AR event dates, we find the maximum IVT for the duration of the AR for every grid cell. \n",
    "    '''\n",
    "    # the final IVT statistic to retain\n",
    "    ivtdir_vals = []\n",
    "    ivt_vals = []\n",
    "    # loop through each AR track\n",
    "    for i, (arcat, track) in enumerate(zip(df.ar_cat.values, df.trackID.values)):\n",
    "        start = df.start_date.values[i]\n",
    "        end = df.end_date.values[i]\n",
    "        # print('Getting maximum between', start, end)\n",
    "        print(i)\n",
    "        # get bbox based on ar_cat\n",
    "        bnds = domains[int(arcat)-1]\n",
    "        # select only the time steps for AR event and specified domain\n",
    "        tmp = ds.sel(time=slice(start, end), lat=slice(bnds[2], bnds[3]), lon=slice(bnds[0], bnds[1]))\n",
    "\n",
    "        ### localized IVT maxima during event\n",
    "        # event_max = tmp.where(tmp.ivt==tmp.ivt.max(), drop=True).squeeze()\n",
    "        event_max = tmp.where(tmp.ivt==tmp.ivt.max(), drop=True).squeeze().load() # this was taking too long, decided to load earlier\n",
    "        ## pull IVT and IVTDIR where ivt is max\n",
    "        uvec = event_max.ivtu.values\n",
    "        uvec = units.Quantity(uvec, \"m/s\")\n",
    "        vvec = event_max.ivtv.values\n",
    "        vvec = units.Quantity(vvec, \"m/s\")\n",
    "        ivtdir = mpcalc.wind_direction(uvec, vvec)\n",
    "        ivtdir_vals.append(ivtdir.item())\n",
    "        ivt_vals.append(event_max.ivt.values.tolist())\n",
    "        \n",
    "    final = [ivtdir_vals, ivt_vals]\n",
    "        \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## For each row, calculate the maximum IVT within the region between start and end\n",
    "ivt_final = ar_ivt(merged_data, ivt, domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## attach data to existing df\n",
    "merged_data['ivt'] = ivt_final[1]\n",
    "merged_data['ivtdir'] = ivt_final[0]\n",
    "\n",
    "# # Export dataframe as csv\n",
    "# outfile = path_to_out + 'IVTDIR_IVT_new.csv'     \n",
    "# merged_data.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ar_precip(df, ds, domains, mode):\n",
    "    '''Calculate precipitation statistics for a subregion in a ds and append to dataframe.\n",
    "     Mode is chosen based on calculation. For each range of AR event dates, we calculate the total accumulated precip for every grid cell. \n",
    "     Then we remove all gridcells that had less than 1 mm of rain per event (these are not included in any calc)\n",
    "     Then we weight the gridcells by the cosine of the latitude.\n",
    "     Then based on mode selected, different statistics are retained:\n",
    "         'mean-total' averages all viable gridcells within the subregion and retains this number\n",
    "         'max-total' selects the maximum gridcell value to append\n",
    "         'percentile-total' calcuates the 95th percentile and then averages all the grid cells that exceed this threshold\n",
    "    '''\n",
    "    # the final precip statistic to retain\n",
    "    m1_vals = []\n",
    "\n",
    "    for i, (arcat, track) in enumerate(zip(df.ar_cat.values, df.trackID.values)):\n",
    "        start = df.start_date.values[i]\n",
    "        end = df.end_date.values[i]\n",
    "        # print('Getting maximum between', start, end)\n",
    "        print(i)\n",
    "        # get bbox based on ar_cat\n",
    "        bnds = domains[int(arcat)-1]\n",
    "        # select only the time steps for AR event and specified domain\n",
    "        tmp = ds.sel(time=slice(start, end), lat=slice(bnds[2], bnds[3]), lon=slice(bnds[0], bnds[1]))\n",
    "\n",
    "        ### event-total precipitation per event for every grid cell\n",
    "        tmp = tmp.sum('time')\n",
    "        ### mask out grid cells with less than 1 mm per event\n",
    "        tmp2 = xr.where(cond=(tmp.prec > 1), x=tmp.prec, y=np.nan)\n",
    "\n",
    "        ### area weighted\n",
    "        # tmp = tmp2.weighted(tmp.weights)\n",
    "\n",
    "        if mode == 'mean-total':\n",
    "            ## mode 1: mean-total\n",
    "            # average over gridcells in weighted subregion\n",
    "            mean_tot = tmp.mean(['lat', 'lon'], skipna=True)\n",
    "            # append to list\n",
    "            m1_vals.append(mean_tot.values.tolist())\n",
    "        elif mode == 'max-total':\n",
    "            ## mode 2: max-total\n",
    "            ### localized precip maxima during event\n",
    "            event_max = tmp2.max(['lat', 'lon'])\n",
    "            m1_vals.append(event_max.values.tolist())\n",
    "        elif mode == 'percentile-total':\n",
    "            ## mode 3: percentile-total\n",
    "            ###  get 95th percentile thres\n",
    "            q_thres = tmp2.quantile(0.95, dim=['lat', 'lon'], interpolation='linear')\n",
    "            ## mask out grid cells below threshold\n",
    "            perc_prec = xr.where(cond=(tmp2 > q_thres), x=tmp2, y=np.nan)\n",
    "            # average over all grid cells skipping nans\n",
    "            mean = perc_prec.mean(['lat', 'lon'], skipna=True)\n",
    "            m1_vals.append(mean.values.tolist())\n",
    "\n",
    "        \n",
    "    return m1_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## For each row, calculate the maximum IVT within the region between start and end\n",
    "prec_final = ar_precip(merged_data, prec, domains, 'max-total')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['prec'] = prec_final\n",
    "\n",
    "# # Export dataframe as csv\n",
    "# outfile = path_to_out + 'IVTDIR_IVT_prec.csv'     \n",
    "# merged_data.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "varname = 'zerodegisotherm'\n",
    "domain = 'd01'\n",
    "\n",
    "filename_pattern = path_to_data + 'wrf_hasia/{0}/{1}/daily/out.wrf6km.{1}.daily_*.nc'.format(domain, varname)\n",
    "print(filename_pattern)\n",
    "ds = xr.open_mfdataset(filename_pattern)\n",
    "\n",
    "# Trim date range\n",
    "idx = slice(start_date, end_date)\n",
    "ds = ds.sel(time=idx)\n",
    "\n",
    "# select only months we are interested in\n",
    "ds = select_months_ds(ds, start_mon, end_mon)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load filtered annual climatology and std\n",
    "clim_std = xr.open_dataset(path_to_data + 'wrf_hasia/d01/zerodegisotherm/daily_std_clim_zerodegisotherm.nc')\n",
    "clim_mean = xr.open_dataset(path_to_data + 'wrf_hasia/d01/zerodegisotherm/filtered_daily_mean_clim_zerodegisotherm.nc')\n",
    "\n",
    "## Calculate Anomalies\n",
    "anomalies = ds.groupby('time.dayofyear') - clim_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize AR dates\n",
    "## get normalized start date for each row in the df - need this to create subset list of freezing level days\n",
    "df = merged_data.rename(columns={'start_date': 'date'})\n",
    "df = df.set_index(pd.to_datetime(df['date']))\n",
    "df = select_months_df(df, start_mon, end_mon)\n",
    "df.index = df.index.strftime(\"%Y-%m-%d\")\n",
    "df = df.rename(columns={'date': 'start_date'})\n",
    "df = df.reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of dates that ar is present\n",
    "ar_dates = pd.to_datetime(df['date']).values\n",
    "# subset freezing level to just ar days\n",
    "anomalies = anomalies.sel(time = ar_dates)\n",
    "anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Calculate low freezing (x - mean < - 1.5*std)\n",
    "low_freezing = anomalies.where(anomalies.z.groupby('time.dayofyear') < clim_std.z*-1.)\n",
    "## Calculate high freezing (x - mean > 1.5*std)\n",
    "high_freezing = anomalies.where(anomalies.z.groupby('time.dayofyear') > clim_std.z*1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# make a ds for each subregion\n",
    "ds_low = []\n",
    "ds_high = []\n",
    "for i, dom in enumerate(domains):\n",
    "    tmp = low_freezing.sel(lon=slice(dom[0], dom[1]), lat=slice(dom[2], dom[3]))\n",
    "    ds_low.append(tmp.load())\n",
    "    tmp = high_freezing.sel(lon=slice(dom[0], dom[1]), lat=slice(dom[2], dom[3]))\n",
    "    ds_high.append(tmp.load())\n",
    "ds_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## this version takes the average value in the subregion\n",
    "for i, region in enumerate(region_name):\n",
    "    ## compute low freezing level\n",
    "    x = ds_low[i].z.values\n",
    "    # flatten array to 2D so it is ntimes, nlat*nlon\n",
    "    ntimes, nlats, nlons = x.shape\n",
    "    x = x.reshape(ntimes, nlats*nlons)\n",
    "    \n",
    "    # calculate mean, skipping nans\n",
    "    low = np.nanmean(x, axis=1)\n",
    "    colname = region + '_low'\n",
    "    df[colname] = low\n",
    "\n",
    "    ## compute high freezing level\n",
    "    x = ds_high[i].z.values\n",
    "    # flatten array to 2D so it is ntimes, nlat*nlon\n",
    "    ntimes, nlats, nlons = x.shape\n",
    "    x = x.reshape(ntimes, nlats*nlons)\n",
    "\n",
    "    # count number of True for each time step\n",
    "    high = np.nanmean(x, axis=1)\n",
    "    colname = region + '_high'\n",
    "    df[colname] = high\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i, region in enumerate(region_name):\n",
    "    ## compute low freezing level\n",
    "    x = ds_low[i].z.values\n",
    "    # flatten array to 2D so it is ntimes, nlat*nlon\n",
    "    ntimes, nlats, nlons = x.shape\n",
    "    x = x.reshape(ntimes, nlats*nlons)\n",
    "\n",
    "    # mark True if value is not nan\n",
    "    a = ~np.isnan(x)\n",
    "    # # mark True if any value for each time step is True (aka not nan)\n",
    "    # z = np.any(a, axis=1)\n",
    "    \n",
    "    # count number of True for each time step\n",
    "    low = np.count_nonzero(a, axis=1)\n",
    "    df['low'] = low\n",
    "\n",
    "    ## compute high freezing level\n",
    "    x = ds_high[i].z.values\n",
    "    # flatten array to 2D so it is ntimes, nlat*nlon\n",
    "    ntimes, nlats, nlons = x.shape\n",
    "    x = x.reshape(ntimes, nlats*nlons)\n",
    "\n",
    "    # mark True if value is not nan\n",
    "    a = ~np.isnan(x)\n",
    "    # # mark True if any value for each time step is True (aka not nan)\n",
    "    # z = np.any(a, axis=1)\n",
    "    \n",
    "    # count number of True for each time step\n",
    "    high = np.count_nonzero(a, axis=1)\n",
    "    df['high'] = high\n",
    "    \n",
    "    colname = region + '_freeze'\n",
    "    df[colname] = 0\n",
    "    df.loc[df['low'] > df['high'], colname] = -1\n",
    "    df.loc[df['low'] < df['high'], colname] = 1\n",
    "    \n",
    "    # drop low and high columns\n",
    "    df = df.drop(columns=['low', 'high'])\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframes as csv\n",
    "df.to_csv(path_to_out + '{0}_ivt_ar_types_freezing_level_max_prec_new.csv'.format(ssn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hasia2)",
   "language": "python",
   "name": "hasia2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
