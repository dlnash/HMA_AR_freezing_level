{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This preprocessing notebook takes 6-hourly AR data from tARget v3, and selects only the times where an AR crosses the 1 km threshold in HMA.\n",
    "\n",
    "Using the trackID from "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Python modules\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "import metpy.calc as mpcalc\n",
    "from metpy.units import units\n",
    "\n",
    "# import personal modules\n",
    "\n",
    "# Path to modules\n",
    "sys.path.append('../modules')\n",
    "\n",
    "# Import my modules\n",
    "from ar_funcs import get_topo_mask\n",
    "from timeseries import select_months_ds, select_months_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "server = 'great'\n",
    "path_to_data = '/home/nash/DATA/data/'                                      # project data -- read only\n",
    "path_to_out  = '../out/'       # output files (numerical results, intermediate datafiles) -- read & write\n",
    "path_to_figs = '../figs/'      # figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of AR dates and trackIDs when an AR crosses 1000 m elevation threshold in HMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify ARs using single bound box with elevation mask during DJF\n",
    "bbox = [20, 40, 65, 97] # HMA region\n",
    "elev_thres = 1000.\n",
    "ssn = 'DJF'\n",
    "\n",
    "if ssn == 'DJF':\n",
    "    start_date = '1979-12-01 0:00'\n",
    "    end_date = '2015-02-28 18:00'\n",
    "    start_mon = 12\n",
    "    end_mon = 2\n",
    "if ssn == 'MAM':\n",
    "    start_date = '1980-03-01 0:00'\n",
    "    end_date = '2014-05-31 18:00'\n",
    "    start_mon = 3\n",
    "    end_mon = 5\n",
    "\n",
    "# open ds\n",
    "filename =  path_to_data + 'ar_catalog/globalARcatalog_ERA-Interim_1979-2019_v3.0.nc'\n",
    "ds = xr.open_dataset(filename, chunks={'time': 1460}, engine='netcdf4')\n",
    "ds = ds.squeeze()\n",
    "# remove lev and ens coords\n",
    "ds = ds.reset_coords(names=['lev', 'ens'], drop=True)\n",
    "\n",
    "# select lats, lons, and dates within start_date, end_date and months\n",
    "lat1, lat2, lon1, lon2 = bbox\n",
    "ds = ds.sel(time=slice(start_date, end_date), lat=slice(lat1,lat2), lon=slice(lon1,lon2))\n",
    "ds = select_months_ds(ds, start_mon, end_mon)\n",
    "\n",
    "# add topo mask\n",
    "mask = get_topo_mask(ds.lat, ds.lon) # create a elevation dataset with same grid spacing as ds\n",
    "ds = ds.where(mask.bedrock >= elev_thres) # mask ds where elevation is less than 1000 m\n",
    "\n",
    "# convert dataset to dataframe\n",
    "df = ds.kidmap.to_dataframe(dim_order=['time', 'lat', 'lon'])\n",
    "df = df.dropna(axis='rows')\n",
    "# keep only rows that have trackID\n",
    "trackID = df.groupby('time').kidmap.unique()\n",
    "# trackID # this is all trackIDs that crossed the 1000 m threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_df = trackID.to_frame() # converts to a pandas dataframe\n",
    "id_df = id_df.reset_index() # reset the index\n",
    "id_df = id_df.rename(columns={'time': 'date'}) # rename time column into date\n",
    "id_df = id_df.set_index(pd.to_datetime(id_df['date'])) # reset the index as \"date\"\n",
    "id_df.index = id_df.index.strftime(\"%Y-%m-%d\") # make it so the index date is normalized to daily\n",
    "id_df = id_df.rename(columns={'date': 'time'}) # rename the date column back to time\n",
    "id_df = id_df.reset_index() # remove the index\n",
    "id_df = id_df.explode('kidmap') # explode the dataframe based on trackID\n",
    "# id_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load AR CAT (from Nash et al. 2021)\n",
    "filepath = path_to_out + 'AR-types_ALLDAYS.csv'\n",
    "ar_cat = pd.read_csv(filepath)\n",
    "ar_cat = ar_cat.rename(columns={'Unnamed: 0': 'date'})\n",
    "ar_cat = ar_cat.set_index(pd.to_datetime(ar_cat['date']))\n",
    "ar_cat = select_months_df(ar_cat, start_mon, end_mon)\n",
    "ar_cat.index = ar_cat.index.strftime(\"%Y-%m-%d\")\n",
    "ar_cat = ar_cat.drop(columns=['date'])\n",
    "ar_cat = ar_cat.reset_index()\n",
    "idx = ar_cat['AR_CAT'] > 0\n",
    "ar_cat = ar_cat.loc[idx]\n",
    "\n",
    "# ar_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge id_df with ar_cat\n",
    "merge_ar = pd.merge(id_df, ar_cat, how='outer', on='date')\n",
    "track_ids = merge_ar.kidmap.unique() # get unique list of AR track IDs\n",
    "ar_dates = merge_ar.time.unique() # get unique list of AR date/times (for later)\n",
    "# merge_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trackID</th>\n",
       "      <th>ar_cat</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2861.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1979-12-02 00:00:00</td>\n",
       "      <td>1979-12-02 18:00:00</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2871.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1979-12-09 06:00:00</td>\n",
       "      <td>1979-12-09 18:00:00</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2975.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1979-12-16 12:00:00</td>\n",
       "      <td>1979-12-17 00:00:00</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2988.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1979-12-21 00:00:00</td>\n",
       "      <td>1979-12-22 12:00:00</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3026.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1979-12-24 06:00:00</td>\n",
       "      <td>1979-12-24 12:00:00</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2367</th>\n",
       "      <td>114217.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-01-03 18:00:00</td>\n",
       "      <td>2015-01-04 00:00:00</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2373</th>\n",
       "      <td>114363.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-01-21 06:00:00</td>\n",
       "      <td>2015-01-22 06:00:00</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2374</th>\n",
       "      <td>114402.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-01-29 12:00:00</td>\n",
       "      <td>2015-01-30 18:00:00</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2379</th>\n",
       "      <td>114602.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-02-24 00:00:00</td>\n",
       "      <td>2015-02-27 06:00:00</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>114668.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-02-27 12:00:00</td>\n",
       "      <td>2015-02-27 18:00:00</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>972 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       trackID  ar_cat          start_date            end_date  duration\n",
       "1       2861.0       1 1979-12-02 00:00:00 1979-12-02 18:00:00      18.0\n",
       "2       2871.0       1 1979-12-09 06:00:00 1979-12-09 18:00:00      12.0\n",
       "3       2975.0       1 1979-12-16 12:00:00 1979-12-17 00:00:00      12.0\n",
       "4       2988.0       1 1979-12-21 00:00:00 1979-12-22 12:00:00      36.0\n",
       "5       3026.0       1 1979-12-24 06:00:00 1979-12-24 12:00:00       6.0\n",
       "...        ...     ...                 ...                 ...       ...\n",
       "2367  114217.0       3 2015-01-03 18:00:00 2015-01-04 00:00:00       6.0\n",
       "2373  114363.0       3 2015-01-21 06:00:00 2015-01-22 06:00:00      24.0\n",
       "2374  114402.0       3 2015-01-29 12:00:00 2015-01-30 18:00:00      30.0\n",
       "2379  114602.0       3 2015-02-24 00:00:00 2015-02-27 06:00:00      78.0\n",
       "2380  114668.0       3 2015-02-27 12:00:00 2015-02-27 18:00:00       6.0\n",
       "\n",
       "[972 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create df with trackID, ar_cat, start date, end date, and duration of AR (how long it is within HMA region)\n",
    "ar = []\n",
    "data = []\n",
    "for i in [1, 2, 3]:\n",
    "    idx = (merge_ar.AR_CAT == i)\n",
    "    ar = merge_ar.loc[idx]\n",
    "\n",
    "    for j, ids in enumerate(track_ids):\n",
    "        idx = (ar.kidmap == ids)\n",
    "        tmp = ar.loc[idx]\n",
    "        start = pd.to_datetime(tmp.time.min())\n",
    "        stop = pd.to_datetime(tmp.time.max()) + timedelta(hours=6)\n",
    "        tmp = (stop - start)\n",
    "        duration = tmp.total_seconds()/(3600) # convert to number of hours\n",
    "\n",
    "        data.append([ids, i, start, stop, duration])\n",
    "    \n",
    "duration_df = pd.DataFrame(data, columns=['trackID', 'ar_cat', 'start_date', 'end_date', 'duration'])\n",
    "duration_df = duration_df.dropna()\n",
    "duration_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landslide DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expand_grid(lat,lon):\n",
    "    '''list all combinations of lats and lons using expand_grid(lat,lon)'''\n",
    "    test = [(A,B) for A in lat for B in lon]\n",
    "    test = np.array(test)\n",
    "    test_lat = test[:,0]\n",
    "    test_lon = test[:,1]\n",
    "    full_grid = pd.DataFrame({'lat': test_lat, 'lon': test_lon})\n",
    "    full_grid = full_grid.sort_values(by=['lat','lon'])\n",
    "    full_grid = full_grid.reset_index(drop=True)\n",
    "    return full_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fname = path_to_data + 'CH2_generated_data/Global_Landslide_Catalog_Export.csv' #TODO check this - is it the raw downloaded data?\n",
    "landslide = pd.read_csv(fname)\n",
    "\n",
    "# Select lat/lon grid\n",
    "lonmin = 65\n",
    "lonmax = 100\n",
    "latmin = 20\n",
    "latmax = 42\n",
    "\n",
    "## Select Landslides within Southern Asia region\n",
    "idx = (landslide.latitude >= latmin) & (landslide.latitude <= latmax) & (landslide.longitude >= lonmin) & (landslide.longitude <= lonmax)\n",
    "landslide = landslide.loc[idx]\n",
    "# set event time as index\n",
    "landslide = landslide.set_index(pd.to_datetime(landslide.event_date))\n",
    "# landslide.index = landslide.index.normalize()\n",
    "\n",
    "# select only landslide dates that are between december and may\n",
    "idx = (landslide.index.month >= 12) | (landslide.index.month <= 5)\n",
    "landslide = landslide[idx]\n",
    "\n",
    "# rename and reindex\n",
    "landslide = landslide.rename(columns={\"latitude\": \"lat\", \"longitude\": \"lon\", \"event_date\": \"event_time\"})\n",
    "landslide = landslide.reset_index()\n",
    "\n",
    "# round event time to the nearest 6 hours\n",
    "landslide['time'] = landslide['event_date'].dt.round('6H')\n",
    "landslide = landslide.set_index(pd.to_datetime(landslide.time))\n",
    "\n",
    "# select only landslide dates that are between december and may\n",
    "idx = (landslide.index.month >= 12) | (landslide.index.month <= 5)\n",
    "landslide = landslide[idx]\n",
    "\n",
    "# landslide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now we want to see if there is an AR present at the same time and location as the landslides\n",
    "# open the trackID for ARs\n",
    "filename =  path_to_data + 'ar_catalog/globalARcatalog_ERA-Interim_1979-2019_v3.0.nc'\n",
    "ar = xr.open_dataset(filename, engine='netcdf4')\n",
    "ar = ar.squeeze()\n",
    "\n",
    "# Select months\n",
    "idx = (ar.time.dt.month >= 12) | (ar.time.dt.month <= 5)\n",
    "kid = ar.kidmap.sel(time=idx) # trackID for indexing\n",
    "\n",
    "# slice the dates so both ds match\n",
    "kid = kid.sel(time=slice('1979-12-01 00', '2019-05-31 00:00'))\n",
    "# kid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## for each landslide_id, if the lat/lon falls within an AR, keep that AR ID and landslide ID\n",
    "landslideID = []\n",
    "arID = []\n",
    "landslide_lat = []\n",
    "landslide_lon = []\n",
    "for i, row in landslide.T.iteritems():\n",
    "    t = kid.sel(lat=row['lat'], lon=row['lon'], time=row['time'], method='nearest').values\n",
    "    # print(t)\n",
    "    if t > 0:\n",
    "        landslideID.append(row['event_id'])\n",
    "        arID.append(t)\n",
    "        landslide_lat.append(row['lat'])\n",
    "        landslide_lon.append(row['lon'])\n",
    "        \n",
    "d = {'landslideID': landslideID, 'trackID': arID, \n",
    "     'landslide_lat': landslide_lat, 'landslide_lon': landslide_lon}\n",
    "landslide_df = pd.DataFrame(data=d)\n",
    "# convert the dtype for the trackID column\n",
    "landslide_df = landslide_df.astype({'trackID': 'float64'})\n",
    "\n",
    "# landslide_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge AR duration df and landslide DF\n",
    "merged_data = pd.merge(duration_df, landslide_df, how='outer', on='trackID')\n",
    "# merged_data \n",
    "# note the rows that do not have a date or time \n",
    "# are landslides that are associated with a specific AR that was not considered a \"HMA AR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## test to make sure merged correctly\n",
    "# idx = merged_data.landslideID > 0\n",
    "# test = merged_data[idx]\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trackID</th>\n",
       "      <th>ar_cat</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>duration</th>\n",
       "      <th>landslideID</th>\n",
       "      <th>landslide_lat</th>\n",
       "      <th>landslide_lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2861.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1979-12-02 00:00:00</td>\n",
       "      <td>1979-12-02 18:00:00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2861.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1979-12-01 00:00:00</td>\n",
       "      <td>1979-12-02 00:00:00</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2871.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1979-12-09 06:00:00</td>\n",
       "      <td>1979-12-09 18:00:00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2871.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1979-12-08 06:00:00</td>\n",
       "      <td>1979-12-09 00:00:00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2975.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1979-12-16 12:00:00</td>\n",
       "      <td>1979-12-17 00:00:00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>114196.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2014-12-31 18:00:00</td>\n",
       "      <td>2015-01-03 12:00:00</td>\n",
       "      <td>66.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>114217.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2015-01-03 18:00:00</td>\n",
       "      <td>2015-01-04 00:00:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>114363.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2015-01-21 06:00:00</td>\n",
       "      <td>2015-01-22 06:00:00</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>114402.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2015-01-29 12:00:00</td>\n",
       "      <td>2015-01-30 18:00:00</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>114668.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2015-02-27 12:00:00</td>\n",
       "      <td>2015-02-27 18:00:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>988 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      trackID  ar_cat          start_date            end_date  duration  \\\n",
       "0      2861.0     1.0 1979-12-02 00:00:00 1979-12-02 18:00:00      18.0   \n",
       "1      2861.0     2.0 1979-12-01 00:00:00 1979-12-02 00:00:00      24.0   \n",
       "2      2871.0     1.0 1979-12-09 06:00:00 1979-12-09 18:00:00      12.0   \n",
       "3      2871.0     2.0 1979-12-08 06:00:00 1979-12-09 00:00:00      18.0   \n",
       "4      2975.0     1.0 1979-12-16 12:00:00 1979-12-17 00:00:00      12.0   \n",
       "..        ...     ...                 ...                 ...       ...   \n",
       "983  114196.0     3.0 2014-12-31 18:00:00 2015-01-03 12:00:00      66.0   \n",
       "984  114217.0     3.0 2015-01-03 18:00:00 2015-01-04 00:00:00       6.0   \n",
       "985  114363.0     3.0 2015-01-21 06:00:00 2015-01-22 06:00:00      24.0   \n",
       "986  114402.0     3.0 2015-01-29 12:00:00 2015-01-30 18:00:00      30.0   \n",
       "987  114668.0     3.0 2015-02-27 12:00:00 2015-02-27 18:00:00       6.0   \n",
       "\n",
       "     landslideID  landslide_lat  landslide_lon  \n",
       "0            NaN            NaN            NaN  \n",
       "1            NaN            NaN            NaN  \n",
       "2            NaN            NaN            NaN  \n",
       "3            NaN            NaN            NaN  \n",
       "4            NaN            NaN            NaN  \n",
       "..           ...            ...            ...  \n",
       "983          NaN            NaN            NaN  \n",
       "984          NaN            NaN            NaN  \n",
       "985          NaN            NaN            NaN  \n",
       "986          NaN            NaN            NaN  \n",
       "987          NaN            NaN            NaN  \n",
       "\n",
       "[988 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the rows that are not a HMA AR\n",
    "idx = merged_data['ar_cat'] > 0\n",
    "merged_data = merged_data.loc[idx]\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ar_ivt(df, ds, domains, clim_mean, clim_std):\n",
    "    '''Calculate maximum IVT for a subregion in a ds and append to dataframe.\n",
    "     For each range of AR event dates, we find the maximum IVT for the duration of the AR for every grid cell. \n",
    "    '''\n",
    "    # the final IVT statistic to retain\n",
    "    ivtdir_vals = []\n",
    "    ivt_vals = []\n",
    "    freeze_vals = []\n",
    "    # loop through each AR track\n",
    "    for i, (arcat, track) in enumerate(zip(df.ar_cat.values, df.trackID.values)):\n",
    "        start = df.start_date.values[i]\n",
    "        end = df.end_date.values[i]\n",
    "        # print('Getting maximum between', start, end)\n",
    "        print(start, end)\n",
    "        # get bbox based on ar_cat\n",
    "        bnds = domains[int(arcat)-1]\n",
    "        # select only the time steps for AR event and specified domain\n",
    "        tmp = ds.sel(time=slice(start, end), lat=slice(bnds[2], bnds[3]), lon=slice(bnds[0], bnds[1]))\n",
    "        \n",
    "        ## remove climatology from freezing level \n",
    "        clim_ar = clim_mean.sel(AR_CAT = arcat, lat=slice(bnds[2], bnds[3]), lon=slice(bnds[0], bnds[1]))\n",
    "        std_ar = clim_std.sel(AR_CAT = arcat, lat=slice(bnds[2], bnds[3]), lon=slice(bnds[0], bnds[1]))\n",
    "        tmp['z_new'] = (tmp.z - clim_ar.z)/std_ar.z # standardized anomalies\n",
    "        ## average freezing level over time, lat, lon\n",
    "        freeze = tmp['z_new'].max(['time'], skipna=True).mean(['lat', 'lon']).values\n",
    "        freeze_vals.append(freeze.tolist())\n",
    "        \n",
    "        ### localized IVT maxima during event\n",
    "        # event_max = tmp.where(tmp.ivt==tmp.ivt.max(), drop=True).squeeze()\n",
    "        event_max = tmp.where(tmp.ivt==tmp.ivt.max(), drop=True).squeeze().load() # this was taking too long, decided to load earlier\n",
    "        ## pull IVT and IVTDIR where ivt is max\n",
    "        uvec = event_max.ivtu.values\n",
    "        uvec = units.Quantity(uvec, \"m/s\")\n",
    "        vvec = event_max.ivtv.values\n",
    "        vvec = units.Quantity(vvec, \"m/s\")\n",
    "        ivtdir = mpcalc.wind_direction(uvec, vvec)\n",
    "        ivtdir_vals.append(ivtdir.item())\n",
    "        ivt_vals.append(event_max.ivt.values.tolist())\n",
    "        \n",
    "        # # pull freezing level anomaly where ivt is max\n",
    "        # freeze_vals.append(event_max.z_new.values)\n",
    "        \n",
    "    final = [ivtdir_vals, ivt_vals, freeze_vals]\n",
    "        \n",
    "    return final\n",
    "\n",
    "def ar_precip(df, ds, domains, mode):\n",
    "    '''Calculate precipitation statistics for a subregion in a ds and append to dataframe.\n",
    "     Mode is chosen based on calculation. For each range of AR event dates, we calculate the total accumulated precip for every grid cell. \n",
    "     Then we remove all gridcells that had less than 1 mm of rain per event (these are not included in any calc)\n",
    "     Then we weight the gridcells by the cosine of the latitude.\n",
    "     Then based on mode selected, different statistics are retained:\n",
    "         'mean-total' averages all viable gridcells within the subregion and retains this number\n",
    "         'max-total' selects the maximum gridcell value to append\n",
    "         'percentile-total' calcuates the 95th percentile and then averages all the grid cells that exceed this threshold\n",
    "    '''\n",
    "    # the final precip statistic to retain\n",
    "    m1_vals = []\n",
    "\n",
    "    for i, (arcat, track) in enumerate(zip(df.ar_cat.values, df.trackID.values)):\n",
    "        start = df.start_date.values[i]\n",
    "        end = df.end_date.values[i]\n",
    "        # print('Getting maximum between', start, end)\n",
    "        print(i)\n",
    "        # get bbox based on ar_cat\n",
    "        bnds = domains[int(arcat)-1]\n",
    "        # select only the time steps for AR event and specified domain\n",
    "        tmp = ds.sel(time=slice(start, end), lat=slice(bnds[2], bnds[3]), lon=slice(bnds[0], bnds[1]))\n",
    "\n",
    "        ### event-total precipitation per event for every grid cell\n",
    "        tmp = tmp.sum('time')\n",
    "        ### mask out grid cells with less than 1 mm per event\n",
    "        tmp2 = xr.where(cond=(tmp.prec > 1), x=tmp.prec, y=np.nan)\n",
    "\n",
    "        ### area weighted\n",
    "        # tmp = tmp2.weighted(tmp.weights)\n",
    "\n",
    "        if mode == 'mean-total':\n",
    "            ## mode 1: mean-total\n",
    "            # average over gridcells in weighted subregion\n",
    "            mean_tot = tmp.mean(['lat', 'lon'], skipna=True)\n",
    "            # append to list\n",
    "            m1_vals.append(mean_tot.values.tolist())\n",
    "        elif mode == 'max-total':\n",
    "            ## mode 2: max-total\n",
    "            ### localized precip maxima during event\n",
    "            event_max = tmp2.max(['lat', 'lon'])\n",
    "            m1_vals.append(event_max.values.tolist())\n",
    "        elif mode == 'percentile-total':\n",
    "            ## mode 3: percentile-total\n",
    "            ###  get 95th percentile thres\n",
    "            q_thres = tmp2.quantile(0.95, dim=['lat', 'lon'], interpolation='linear')\n",
    "            ## mask out grid cells below threshold\n",
    "            perc_prec = xr.where(cond=(tmp2 > q_thres), x=tmp2, y=np.nan)\n",
    "            # average over all grid cells skipping nans\n",
    "            mean = perc_prec.mean(['lat', 'lon'], skipna=True)\n",
    "            m1_vals.append(mean.values.tolist())\n",
    "\n",
    "        \n",
    "    return m1_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load 2D WRF data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get lats and lons from d01 and d02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pull wrflats and wrflons from first file\n",
    "fname = path_to_data + 'wrf_hasia/d01/ivt/3hr/tmp_2015.nc'\n",
    "tmp = xr.open_dataset(fname)\n",
    "# print(tmp.time[:100])\n",
    "# print(tmp.time[-100:])\n",
    "\n",
    "## assign those lats to the other ds when you loop\n",
    "wrflats = tmp.lat.values\n",
    "wrflons = tmp.lon.values\n",
    "\n",
    "fname = path_to_data + 'wrf_hasia/d02/prec/3hr/tmp_2014.nc'\n",
    "tmp = xr.open_dataset(fname)\n",
    "# print(tmp.time[:100])\n",
    "# print(tmp.time[-100:])\n",
    "\n",
    "## assign those lats to the other ds when you loop\n",
    "wrflats2 = tmp.lat.values\n",
    "wrflons2 = tmp.lon.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nash/DATA/data/wrf_hasia/d01/zerodegisotherm/daily/out.wrf6km.zerodegisotherm.daily_*.nc\n",
      "CPU times: user 580 ms, sys: 111 ms, total: 691 ms\n",
      "Wall time: 3.24 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2 {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt;\n",
       "Dimensions:  (time: 3249, lat: 249, lon: 259)\n",
       "Coordinates:\n",
       "  * time     (time) datetime64[ns] 1979-12-01 1979-12-02 ... 2015-02-28\n",
       "  * lat      (lat) float32 16.43 16.63 16.83 17.02 ... 55.8 55.92 56.04 56.15\n",
       "  * lon      (lon) float32 60.2 60.41 60.62 60.83 ... 113.2 113.4 113.6 113.8\n",
       "Data variables:\n",
       "    z        (time, lat, lon) float32 dask.array&lt;chunksize=(31, 249, 259), meta=np.ndarray&gt;</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-ca8dda05-9aa2-470b-9455-98daaed2de45' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-ca8dda05-9aa2-470b-9455-98daaed2de45' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>time</span>: 3249</li><li><span class='xr-has-index'>lat</span>: 249</li><li><span class='xr-has-index'>lon</span>: 259</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-99476e1b-6eed-42fe-b050-2b59ff0d36d8' class='xr-section-summary-in' type='checkbox'  checked><label for='section-99476e1b-6eed-42fe-b050-2b59ff0d36d8' class='xr-section-summary' >Coordinates: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>1979-12-01 ... 2015-02-28</div><input id='attrs-5207ed88-9ec3-420d-a5e5-a6689401138d' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-5207ed88-9ec3-420d-a5e5-a6689401138d' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-c894b0a1-52b3-4c10-8169-d608027b7600' class='xr-var-data-in' type='checkbox'><label for='data-c894b0a1-52b3-4c10-8169-d608027b7600' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([&#x27;1979-12-01T00:00:00.000000000&#x27;, &#x27;1979-12-02T00:00:00.000000000&#x27;,\n",
       "       &#x27;1979-12-03T00:00:00.000000000&#x27;, ..., &#x27;2015-02-26T00:00:00.000000000&#x27;,\n",
       "       &#x27;2015-02-27T00:00:00.000000000&#x27;, &#x27;2015-02-28T00:00:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lat</span></div><div class='xr-var-dims'>(lat)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>16.43 16.63 16.83 ... 56.04 56.15</div><input id='attrs-81903552-a733-454e-b69b-826923762f62' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-81903552-a733-454e-b69b-826923762f62' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-fa78a5d8-fd3a-4943-bee2-55872855369b' class='xr-var-data-in' type='checkbox'><label for='data-fa78a5d8-fd3a-4943-bee2-55872855369b' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([16.428108, 16.62725 , 16.82618 , ..., 55.921158, 56.037384, 56.15326 ],\n",
       "      dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lon</span></div><div class='xr-var-dims'>(lon)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>60.2 60.41 60.62 ... 113.6 113.8</div><input id='attrs-4025e122-c218-4945-aebb-0d5666880774' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-4025e122-c218-4945-aebb-0d5666880774' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-67faadd5-c222-4b6e-9748-540a5732f42b' class='xr-var-data-in' type='checkbox'><label for='data-67faadd5-c222-4b6e-9748-540a5732f42b' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([ 60.203857,  60.41158 ,  60.6193  , ..., 113.3807  , 113.588425,\n",
       "       113.79614 ], dtype=float32)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-d3046372-8d17-44db-949d-74913d2af16f' class='xr-section-summary-in' type='checkbox'  checked><label for='section-d3046372-8d17-44db-949d-74913d2af16f' class='xr-section-summary' >Data variables: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>z</span></div><div class='xr-var-dims'>(time, lat, lon)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>dask.array&lt;chunksize=(31, 249, 259), meta=np.ndarray&gt;</div><input id='attrs-fc954f04-adc2-46a0-bd7a-8acc0d2d22f7' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-fc954f04-adc2-46a0-bd7a-8acc0d2d22f7' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-eec68dae-d260-416c-bbf8-4fe82c517649' class='xr-var-data-in' type='checkbox'><label for='data-eec68dae-d260-416c-bbf8-4fe82c517649' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 838.13 MB </td> <td> 23.47 MB </td></tr>\n",
       "    <tr><th> Shape </th><td> (3249, 249, 259) </td> <td> (91, 249, 259) </td></tr>\n",
       "    <tr><th> Count </th><td> 185 Tasks </td><td> 37 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> float32 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"167\" height=\"157\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"80\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"10\" y1=\"37\" x2=\"80\" y2=\"107\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"37\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"37\" />\n",
       "  <line x1=\"14\" y1=\"4\" x2=\"14\" y2=\"41\" />\n",
       "  <line x1=\"18\" y1=\"8\" x2=\"18\" y2=\"45\" />\n",
       "  <line x1=\"22\" y1=\"12\" x2=\"22\" y2=\"49\" />\n",
       "  <line x1=\"26\" y1=\"16\" x2=\"26\" y2=\"53\" />\n",
       "  <line x1=\"30\" y1=\"20\" x2=\"30\" y2=\"57\" />\n",
       "  <line x1=\"34\" y1=\"24\" x2=\"34\" y2=\"61\" />\n",
       "  <line x1=\"38\" y1=\"28\" x2=\"38\" y2=\"65\" />\n",
       "  <line x1=\"42\" y1=\"32\" x2=\"42\" y2=\"69\" />\n",
       "  <line x1=\"45\" y1=\"35\" x2=\"45\" y2=\"73\" />\n",
       "  <line x1=\"49\" y1=\"39\" x2=\"49\" y2=\"76\" />\n",
       "  <line x1=\"53\" y1=\"43\" x2=\"53\" y2=\"80\" />\n",
       "  <line x1=\"57\" y1=\"47\" x2=\"57\" y2=\"84\" />\n",
       "  <line x1=\"61\" y1=\"51\" x2=\"61\" y2=\"88\" />\n",
       "  <line x1=\"65\" y1=\"55\" x2=\"65\" y2=\"92\" />\n",
       "  <line x1=\"69\" y1=\"59\" x2=\"69\" y2=\"96\" />\n",
       "  <line x1=\"73\" y1=\"63\" x2=\"73\" y2=\"100\" />\n",
       "  <line x1=\"77\" y1=\"67\" x2=\"77\" y2=\"104\" />\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"80\" y2=\"107\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 80.58823529411765,70.58823529411765 80.58823529411765,107.66366680678564 10.0,37.075431512668\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"47\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"47\" y2=\"0\" />\n",
       "  <line x1=\"14\" y1=\"4\" x2=\"51\" y2=\"4\" />\n",
       "  <line x1=\"18\" y1=\"8\" x2=\"55\" y2=\"8\" />\n",
       "  <line x1=\"22\" y1=\"12\" x2=\"59\" y2=\"12\" />\n",
       "  <line x1=\"26\" y1=\"16\" x2=\"63\" y2=\"16\" />\n",
       "  <line x1=\"30\" y1=\"20\" x2=\"67\" y2=\"20\" />\n",
       "  <line x1=\"34\" y1=\"24\" x2=\"71\" y2=\"24\" />\n",
       "  <line x1=\"38\" y1=\"28\" x2=\"75\" y2=\"28\" />\n",
       "  <line x1=\"42\" y1=\"32\" x2=\"79\" y2=\"32\" />\n",
       "  <line x1=\"45\" y1=\"35\" x2=\"83\" y2=\"35\" />\n",
       "  <line x1=\"49\" y1=\"39\" x2=\"87\" y2=\"39\" />\n",
       "  <line x1=\"53\" y1=\"43\" x2=\"91\" y2=\"43\" />\n",
       "  <line x1=\"57\" y1=\"47\" x2=\"95\" y2=\"47\" />\n",
       "  <line x1=\"61\" y1=\"51\" x2=\"98\" y2=\"51\" />\n",
       "  <line x1=\"65\" y1=\"55\" x2=\"102\" y2=\"55\" />\n",
       "  <line x1=\"69\" y1=\"59\" x2=\"106\" y2=\"59\" />\n",
       "  <line x1=\"73\" y1=\"63\" x2=\"110\" y2=\"63\" />\n",
       "  <line x1=\"77\" y1=\"67\" x2=\"114\" y2=\"67\" />\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"117\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"80\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"47\" y1=\"0\" x2=\"117\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 47.305899619074175,0.0 117.89413491319183,70.58823529411765 80.58823529411765,70.58823529411765\" style=\"fill:#8B4903A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"117\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"80\" y1=\"107\" x2=\"117\" y2=\"107\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"80\" y2=\"107\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"117\" y1=\"70\" x2=\"117\" y2=\"107\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"80.58823529411765,70.58823529411765 117.89413491319183,70.58823529411765 117.89413491319183,107.66366680678564 80.58823529411765,107.66366680678564\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"99.241185\" y=\"127.663667\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >259</text>\n",
       "  <text x=\"137.894135\" y=\"89.125951\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,137.894135,89.125951)\">249</text>\n",
       "  <text x=\"35.294118\" y=\"92.369549\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,35.294118,92.369549)\">3249</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table></div></li></ul></div></li><li class='xr-section-item'><input id='section-8791ec1a-3996-46a7-8de7-afa81482c93c' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-8791ec1a-3996-46a7-8de7-afa81482c93c' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (time: 3249, lat: 249, lon: 259)\n",
       "Coordinates:\n",
       "  * time     (time) datetime64[ns] 1979-12-01 1979-12-02 ... 2015-02-28\n",
       "  * lat      (lat) float32 16.43 16.63 16.83 17.02 ... 55.8 55.92 56.04 56.15\n",
       "  * lon      (lon) float32 60.2 60.41 60.62 60.83 ... 113.2 113.4 113.6 113.8\n",
       "Data variables:\n",
       "    z        (time, lat, lon) float32 dask.array<chunksize=(31, 249, 259), meta=np.ndarray>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "varname = 'zerodegisotherm'\n",
    "domain = 'd01'\n",
    "\n",
    "filename_pattern = path_to_data + 'wrf_hasia/{0}/{1}/daily/out.wrf6km.{1}.daily_*.nc'.format(domain, varname)\n",
    "print(filename_pattern)\n",
    "ds = xr.open_mfdataset(filename_pattern)\n",
    "\n",
    "# Trim date range\n",
    "idx = slice(start_date, end_date)\n",
    "ds = ds.sel(time=idx)\n",
    "\n",
    "# select only months we are interested in\n",
    "ds = select_months_ds(ds, start_mon, end_mon)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading... ivt\n",
      "Reading... prec\n",
      "Reading... zerodegisotherm\n",
      "CPU times: user 2.3 s, sys: 102 ms, total: 2.4 s\n",
      "Wall time: 2.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def preprocess_ivt(ds):\n",
    "    '''keep only the current year'''\n",
    "    year = ds.time.dt.year.max().values\n",
    "    ds = ds.assign_coords({\"lon\": wrflons, \"lat\": wrflats})\n",
    "    if year == 1980:\n",
    "        ds = ds\n",
    "    else:\n",
    "        ds = ds.sel(time=slice('{0}-01-01 00:00'.format(year), '{0}-12-31 21:00'.format(year)))\n",
    "    return ds\n",
    "\n",
    "def preprocess_prec(ds):\n",
    "    '''keep only the current year'''\n",
    "    year = ds.time.dt.year.max().values\n",
    "    ds = ds.assign_coords({\"lon\": wrflons2, \"lat\": wrflats2})\n",
    "    if year == 1980:\n",
    "        ds = ds\n",
    "    else:\n",
    "        ds = ds.sel(time=slice('{0}-01-01 00:00'.format(year), '{0}-12-31 21:00'.format(year)))\n",
    "    return ds\n",
    "\n",
    "domains = ['d01', 'd02', 'd01']\n",
    "varname_lst = ['ivt', 'prec', 'zerodegisotherm']\n",
    "\n",
    "## loop through each ds\n",
    "ds_lst = []\n",
    "for i, (dom, varname) in enumerate(zip(domains, varname_lst)):\n",
    "    print('Reading...', varname)\n",
    "    if server == 'great':\n",
    "        data_path = path_to_data + 'wrf_hasia/'\n",
    "    else:\n",
    "        data_path = path_to_data + 'wrf_preprocessed_data/wrf_6km/'\n",
    "        \n",
    "    filename_pattern = '{0}/{1}/3hr/tmp_*.nc'.format(dom, varname)\n",
    "    fname = data_path + filename_pattern\n",
    "    \n",
    "    if varname == 'ivt':\n",
    "        ds = xr.open_mfdataset(fname, preprocess=preprocess_ivt)\n",
    "        ds = ds.assign(ivt=lambda ds: np.sqrt(ds.ivtu**2 + ds.ivtv**2))\n",
    "    elif varname == 'prec':\n",
    "        ds = xr.open_mfdataset(fname, preprocess=preprocess_prec)\n",
    "        ## shift subtraction to get mm per hour \n",
    "        # # rain at next time step - rain at current time step\n",
    "        ds = ds.shift(time=-1) - ds # if in xarray\n",
    "    elif varname == 'geopotential':\n",
    "        ds = ds.sel(lev=250.)\n",
    "    elif varname == 'zerodegisotherm':\n",
    "        ds = xr.open_mfdataset(fname, preprocess=preprocess_ivt)\n",
    "    \n",
    "    # subset to just ar days\n",
    "    # ds = ds.sel(time = slice(start_date, end_date))\n",
    "    # ds = select_months_ds(ds, start_mon, end_mon)\n",
    "    # ds = ds.sel(time = ar_dates[:-1])\n",
    "    \n",
    "    ds_lst.append(ds)\n",
    "    \n",
    "ivt = ds_lst[0]\n",
    "prec = ds_lst[1]\n",
    "zerodeg = ds_lst[2]\n",
    "\n",
    "wrf_d01 = xr.combine_by_coords([ds_lst[0], ds_lst[2]]) ## combine IVT and freezing level into 1 ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dates</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dates</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [dates]\n",
       "Index: []"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Having trouble with wrf ds not having all the dates in the ar_dates list \n",
    "## use this to find out which dates are having a problem\n",
    "# make a pandas dataframe of AR Dates\n",
    "d = {'dates': ar_dates[:-1]}\n",
    "df_A = pd.DataFrame(data=d)\n",
    "df_A = df_A.set_index(pd.to_datetime(df_A['dates'])) # reset the index as \"dates\"\n",
    "\n",
    "\n",
    "# make a pandas dataframe of WRF dates\n",
    "d = {'dates': zerodeg.time}\n",
    "df_B = pd.DataFrame(data=d)\n",
    "df_B = df_B.set_index(pd.to_datetime(df_B['dates'])) # reset the index as \"dates\"\n",
    "\n",
    "# test = df_A.isin(df_B)\n",
    "\n",
    "x = df_A.index\n",
    "y = df_B.index\n",
    "test = x.isin(y)\n",
    "\n",
    "idx = (test== False)\n",
    "df_A.loc[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing IVT and freezing level...\n",
      "1979-12-02T00:00:00.000000000 1979-12-02T18:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbarc/students/nash/miniconda3/envs/hasia2/lib/python3.7/site-packages/dask/utils.py:35: RuntimeWarning: All-NaN slice encountered\n",
      "  return func(*args, **kwargs)\n",
      "/home/sbarc/students/nash/miniconda3/envs/hasia2/lib/python3.7/site-packages/dask/core.py:121: RuntimeWarning: All-NaN slice encountered\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1979-12-01T00:00:00.000000000 1979-12-02T00:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbarc/students/nash/miniconda3/envs/hasia2/lib/python3.7/site-packages/dask/utils.py:35: RuntimeWarning: All-NaN slice encountered\n",
      "  return func(*args, **kwargs)\n",
      "/home/sbarc/students/nash/miniconda3/envs/hasia2/lib/python3.7/site-packages/dask/core.py:121: RuntimeWarning: All-NaN slice encountered\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1979-12-09T06:00:00.000000000 1979-12-09T18:00:00.000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbarc/students/nash/miniconda3/envs/hasia2/lib/python3.7/site-packages/dask/utils.py:35: RuntimeWarning: All-NaN slice encountered\n",
      "  return func(*args, **kwargs)\n",
      "/home/sbarc/students/nash/miniconda3/envs/hasia2/lib/python3.7/site-packages/dask/core.py:121: RuntimeWarning: All-NaN slice encountered\n",
      "  return func(*(_execute_task(a, cache) for a in args))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "CPU times: user 3.96 s, sys: 5.47 s, total: 9.43 s\n",
      "Wall time: 9.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# latmin, latmax, lonmin, lonmax\n",
    "ext1 = [71, 79, 32, 37] # Western precip anomalies\n",
    "ext2 = [69, 74, 37, 40] # Northwestern precip anomalies\n",
    "ext3 = [90, 99, 24, 30] # Eastern precip anomalies\n",
    "\n",
    "region_name = ['western', 'northwestern', 'eastern']\n",
    "domains = [ext1, ext2, ext3]\n",
    "clim_mean = xr.open_dataset('/home/sbarc/students/nash/data/wrf_hasia/d01/zerodegisotherm_ar_clim_new.nc') # freezing level climatology\n",
    "clim_std = xr.open_dataset('/home/sbarc/students/nash/data/wrf_hasia/d01/zerodegisotherm_ar_std_new.nc') # freezing level standard deviation\n",
    "\n",
    "print('Processing IVT and freezing level...')\n",
    "## For each row, calculate the maximum IVT within the region between start and end\n",
    "ivt_final = ar_ivt(merged_data.iloc[0:3], wrf_d01, domains, clim_mean, clim_std)\n",
    "\n",
    "## For each row, calculate the maximum precip within the region between start and end\n",
    "prec_final = ar_precip(merged_data.iloc[0:3], prec, domains, 'max-total')\n",
    "\n",
    "## attach data to existing df\n",
    "# merged_data['ivt'] = ivt_final[1]\n",
    "# merged_data['ivtdir'] = ivt_final[0]\n",
    "# merged_data['freeze'] = ivt_final[2]\n",
    "# merged_data['prec'] = prec_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5622614622116089, 1.7191874980926514, 1.795113205909729]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ivt_final[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load filtered annual climatology and std\n",
    "clim_std = xr.open_dataset(path_to_data + 'wrf_hasia/d01/zerodegisotherm/daily_std_clim_zerodegisotherm.nc')\n",
    "clim_mean = xr.open_dataset(path_to_data + 'wrf_hasia/d01/zerodegisotherm/filtered_daily_mean_clim_zerodegisotherm.nc')\n",
    "\n",
    "\n",
    "clim_mean = clim_mean.sel(AR_CAT = 1)\n",
    "\n",
    "## Calculate Anomalies\n",
    "anomalies = ds.groupby('time.dayofyear') - clim_mean\n",
    "\n",
    "# normalize AR dates\n",
    "## get normalized start date for each row in the df - need this to create subset list of freezing level days\n",
    "df = merged_data.rename(columns={'start_date': 'date'})\n",
    "df = df.set_index(pd.to_datetime(df['date']))\n",
    "df = select_months_df(df, start_mon, end_mon)\n",
    "df.index = df.index.strftime(\"%Y-%m-%d\")\n",
    "df = df.rename(columns={'date': 'start_date'})\n",
    "df = df.reset_index()\n",
    "df\n",
    "\n",
    "# get list of dates that ar is present\n",
    "ar_dates = pd.to_datetime(df['date']).values\n",
    "# subset freezing level to just ar days\n",
    "anomalies = anomalies.sel(time = ar_dates)\n",
    "anomalies\n",
    "\n",
    "## Calculate low freezing (x - mean < - 1.5*std)\n",
    "low_freezing = anomalies.where(anomalies.z.groupby('time.dayofyear') < clim_std.z*-1.)\n",
    "## Calculate high freezing (x - mean > 1.5*std)\n",
    "high_freezing = anomalies.where(anomalies.z.groupby('time.dayofyear') > clim_std.z*1.)\n",
    "\n",
    "# make a ds for each subregion\n",
    "ds_low = []\n",
    "ds_high = []\n",
    "for i, dom in enumerate(domains):\n",
    "    tmp = low_freezing.sel(lon=slice(dom[0], dom[1]), lat=slice(dom[2], dom[3]))\n",
    "    ds_low.append(tmp.load())\n",
    "    tmp = high_freezing.sel(lon=slice(dom[0], dom[1]), lat=slice(dom[2], dom[3]))\n",
    "    ds_high.append(tmp.load())\n",
    "ds_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # make a ds for each subregion\n",
    "# ds_lst = []\n",
    "# for i, bnds in enumerate(domains):\n",
    "#     tmp = ivt.sel(lat=slice(bnds[2], bnds[3]), lon=slice(bnds[0], bnds[1])) \n",
    "#     ds_lst.append(tmp)\n",
    "# ds_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## this version takes the average value in the subregion\n",
    "for i, region in enumerate(region_name):\n",
    "    ## compute low freezing level\n",
    "    x = ds_low[i].z.values\n",
    "    # flatten array to 2D so it is ntimes, nlat*nlon\n",
    "    ntimes, nlats, nlons = x.shape\n",
    "    x = x.reshape(ntimes, nlats*nlons)\n",
    "    \n",
    "    # calculate mean, skipping nans\n",
    "    low = np.nanmean(x, axis=1)\n",
    "    colname = region + '_low'\n",
    "    df[colname] = low\n",
    "\n",
    "    ## compute high freezing level\n",
    "    x = ds_high[i].z.values\n",
    "    # flatten array to 2D so it is ntimes, nlat*nlon\n",
    "    ntimes, nlats, nlons = x.shape\n",
    "    x = x.reshape(ntimes, nlats*nlons)\n",
    "\n",
    "    # count number of True for each time step\n",
    "    high = np.nanmean(x, axis=1)\n",
    "    colname = region + '_high'\n",
    "    df[colname] = high\n",
    "\n",
    "\n",
    "df\n",
    "\n",
    "## this version counts the number of grid cells in ds_high and ds_low\n",
    "## if the number of ds_high gricells > ds_low gridcells = above average day\n",
    "## if the number of ds_low > ds_high gricells = below average day\n",
    "for i, region in enumerate(region_name):\n",
    "    ## compute low freezing level\n",
    "    x = ds_low[i].z.values\n",
    "    # flatten array to 2D so it is ntimes, nlat*nlon\n",
    "    ntimes, nlats, nlons = x.shape\n",
    "    x = x.reshape(ntimes, nlats*nlons)\n",
    "\n",
    "    # mark True if value is not nan\n",
    "    a = ~np.isnan(x)\n",
    "    # # mark True if any value for each time step is True (aka not nan)\n",
    "    # z = np.any(a, axis=1)\n",
    "    \n",
    "    # count number of True for each time step\n",
    "    low = np.count_nonzero(a, axis=1)\n",
    "    df['low'] = low\n",
    "\n",
    "    ## compute high freezing level\n",
    "    x = ds_high[i].z.values\n",
    "    # flatten array to 2D so it is ntimes, nlat*nlon\n",
    "    ntimes, nlats, nlons = x.shape\n",
    "    x = x.reshape(ntimes, nlats*nlons)\n",
    "\n",
    "    # mark True if value is not nan\n",
    "    a = ~np.isnan(x)\n",
    "    # # mark True if any value for each time step is True (aka not nan)\n",
    "    # z = np.any(a, axis=1)\n",
    "    \n",
    "    # count number of True for each time step\n",
    "    high = np.count_nonzero(a, axis=1)\n",
    "    df['high'] = high\n",
    "    \n",
    "    colname = region + '_freeze'\n",
    "    df[colname] = 0\n",
    "    df.loc[df['low'] > df['high'], colname] = -1\n",
    "    df.loc[df['low'] < df['high'], colname] = 1\n",
    "    \n",
    "    # drop low and high columns\n",
    "    df = df.drop(columns=['low', 'high'])\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframes as csv\n",
    "df.to_csv(path_to_out + '{0}_ivt_ar_types_freezing_level_max_prec_new.csv'.format(ssn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hasia2)",
   "language": "python",
   "name": "hasia2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
